---
title: Spark-基础教程1-分布并行计算
tags:
  - spark
categories:
  - Notes
date: 2015-06-04 11:24:06
---

## 搬砖 vs. 分布式计算

一个人搬砖很累，几个人一起搬就会轻松很多，也会快很多。
分布并行计算和几个人一起搬砖的意思是一致的，一个资源密集型的任务（搬砖或计算），需要 一组资源（小伙伴或计算节点），并行地完成：

- 计算任务 => 搬砖
- 计算节点 => 小伙伴

当计算任务过重时，我们就把计算任务拆分，然后放到多个计算节点上同时执行,这就是分布并行计算。

## 求文件中包含”包租婆”的行数

从一个总计100行的文件中找出所有包含“包租婆”的行数，我们不用太动脑筋就有一个算法：

* 读一行，判断这一行有“包租婆”吗？如果有，全局变量count加1。
* 文件到末尾了吗？如果没有，跳转到第1步继续执行。
* 打印count。

这几步程序，我打赌在你的计算机上可以一眨眼的功夫就执行完。但是如果这个文件有100万行呢？ 如果还用刚才不动脑筋的笨算法，可能就不好交差了……
并行分布计算采用了一个大智若愚的办法，通过将笨算法丢给一群机器同时去算，实现规定时间内规定 任务的完成。你要知道，如今流行的Map/Reduce就是这么干的，这听起来不够高端，也确实引起了一些数据库专 家（聪明人）的非议。不过，不管黑猫白猫，能抓住老鼠的都是好猫。

## Spark简化了分布式计算的开发

如果要把刚才的任务进行分布计算（假设有10台机器可以用），需要对原始的笨算法做一些调整：

- 把100万条数据分成10份，每份10万条。
- 在10台机器上分别执行笨办法计算包含“包租婆”的行数。
- 汇总合并10台机器的计算结果，即count，打印出来。

Oh…NO…..太…累…了…

好在有Spark的存在！我们只要把数据和计算程序交给Spark，它会机智地进行数据切分、算法复制、分布执行、结果合并。

## Spark的计算范式：数据集上的计算

Spark用起来的确简单，但有一点特别要注意，你得按照Spark的范式写算法。
Spark是在数据集的层次上进行分布并行计算，是的，它只认成堆的数据：
我们提交给Spark的计算任务，必须满足两个条件：

- 数据是可以分块的，每块构成一个集合。
- 算法只能在集合级别执行操作。

比如，对于文本文件，在Spark中，一行就是一条记录，若干条记录组成一个集合。我们 原来的算法直接在每一行上进行计算，就不行了。需要先构建数据集，然后通过数据集的操作， 实现我们的目的。

## SQL中的数据集

如果你熟悉SQL，可以用SQL的思维考虑下什么是集合操作：

``` sql
UPDATE USER SET GENDER='FEMALE'
```

上面的SQL语句就是一个集合操作，对一个数据集合，执行一条UPDATE操作，整个数据集都被修改了。
UPDATE语句有两个特点，这也是集合操作的要素：

1、对集合的每个记录执行相同的操作

UPDATE更新了集合中的所有记录，这些记录的 GENDER 字段值都被更新为 FEMALE 。

2、这个操作的具体行为是用户指定的

UPDATE通过SET子句，指定更新那些字段，怎么更新。

## JavaScript中的数据集

JavaScript中数组对象的map方法也是一种集合操作。map方法将一个数组的每一个成员变换为新的成员， 并返回变换后新的集合。

``` javascript
var a=[1,2,3,4];
a.map(function(d){return d*2;});
console.log(a);
```

上面的JavaScript代码对一个数组执行map方法，将每一个成员进行倍乘。结果是获得一个新的 数组，比如在这里，将得到`[2,4,6,8]`。
这个例子也说明了集合操作的两个要素：

1、对集合的每个记录执行相同的操作

在map方法执行中，每个数组成员都被转换为原始值的2倍。

2、这个操作的具体行为是用户指定的

map方法使用一个匿名函数，指定如何对每一个原始数据进行变换。

## 将算法移植到Spark上

现在我们修改原始的笨算法，使之适用于Spark：

将数据载入并构造数据集

在Spark中，这个数据集被称为`RDD` ：`弹性分布数据集`。

## 对数据集进行map操作

指定行为：如果一行原始记录包含“包租婆”，该行记录映射为新值1，否则映射为新值0 。

对map后的数据集进行`collect`操作，获得合并的结果。

上面的map操作，和前面JavaScript数组的map方法类似，将原始记录映射为新的记录，并返回一个`新的RDD`。 collect操作提取RDD中的全部数据到本地。

魔术发生在RDD上。Spark的RDD自动进行数据的切分和结果的整合。我们假装不知道就好了， 就像这一切只发生在本地的一台机器上。

## Spark操作符

Spark提供了80多种操作符对集合进行操作。我们列举常用的一些供你建立一点基本概念， 以便了解Spark可以支持什么：

### 变换

变换操作总是获得一个新的RDD:

`map(func)` : 将原始数据集的每一个记录使用传入的函数func ，映射为一个新的记录，并返回新的RDD。
`filter(func)` : 返回一个新的RDD，仅包含那些符合条件的记录，即func返回true 。
`flatMap(func)` : 和map类似，只是原始记录的一条可能被映射为新的RDD中的多条。
`union(otherDataset)` : 合并两个RDD，返回一个新的RDD 。
`intersection(otherDataset)`：返回一个新的RDD，仅包含两个RDD共有的记录。

### 动作

动作操作总是获得一个本地数据，这意味着控制权回到你的程序了:

`reduce(func)` : 使用func对RDD的记录进行聚合。
`collect()` : 返回RDD中的所有记录
`count()` : 返回RDD中的记录总数